{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fatality Analysis Reporting System\n",
    "**Dataset description:** <br>\n",
    "The FARS (Fatality Analysis Reporting System) data set is a compilation of statistics about car accidents made by the U.S. National Center for Statistics and Analysis.\n",
    "The specific dataset used contains information about all of the people involved in car accidents in the U.S. during 2001, where most of its attributes are represented with nominal values. The class attribute describes the level of injury suffered.\n",
    "- Type - Classification\n",
    "- Size - 67312 instances\n",
    "- Features - 29\n",
    "- Classes - 8\n",
    "\n",
    "**This is an interactive notebook!**\n",
    "- Many cells written in this notebook are meant for re-use, so you can explore the data set by yourself step-by-step, then go back and see the effect running a cell had on the previous ones. \n",
    "- If however you just want to see the results, you can run the whole notebook with no problem. If you don't have the required packages and you don't want to install them, you can view the complete notebook and trained model on my [GitHub account](https://github.com/danbochman/FARS_LEARNING).\n",
    "\n",
    "\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Data Processing\n",
    "The dataset came in a .out format, I processed it in Excel to reconstruct it to tabular form, and saved it as an .csv file\n",
    "\n",
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fars_train_proc.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main obstacles that need to be addressed:**\n",
    "1. Most of the columns' values are non-numeric, we need to encode it or use an algorithm that can deal with it efficiently\n",
    "2. There are many values that are relevant only for certain conditions, for example: DRUG_TEST_RESULTS_(2_of_3) is relevant only if DRUG_TEST_TYPE_(2_of_3) was conducted. Zero values in these columns are actually N\\A values.\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "First let's look for candidates for feature merging or dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classes balance\n",
    "df['INJURY_SEVERITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Died_Prior_to Accident** shows only 6 times out of 100967 data examples. This classification is a lost cause and it's better to drop it.\n",
    "- **Injured_Severity_Unknown**, **Unknown** and **Possible_Injury** practically mean the same thing. It doesn't make sense to train the algorithm to predict when injury severities will be unknown, when in doubt - there's a possibility for injury. I would merge the terms to make a more robust class. By doing so we'll also gain more balance between the classes count, avoiding skewed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class merge and drop\n",
    "df.INJURY_SEVERITY.replace(['Unknown', 'Injured_Severity_Unknown'], 'Possible_Injury', inplace=True) # merge classes\n",
    "df = df[df.INJURY_SEVERITY != 'Died_Prior_to_Accident'] # drop class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of distinct occurrences for values in a column \n",
    "df['SEX'].value_counts() # Memo:  SEX, ALCOHOL_TEST_RESULT, DRUG_TEST_RESULTS_(1_of_3), HISPANIC_ORIGIN, RACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate correlation between a certain category value and injury severity\n",
    "category = 'SEX'                                     # Memo: 'RACE' - 'Not_a_Fatality_(Not_Applicable)'\n",
    "value = 'Unknown'\n",
    "dcorr = df.loc[df[category] == value]     # For specific category\n",
    "# dcorr = train_data.loc[df['column_name'].isin(some_values)] # For several categories\n",
    "\n",
    "print('Injury severity distribution for ' + category + \" - \" + str(value))\n",
    "dcorr['INJURY_SEVERITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing the data from police reports, test types and test results\n",
    "I contemplated much about what to do with police reports and test types for alcohol\\drug involvement, they seem like redundant features next to the numeric test results that already convey this information.The biggest problem arises when a person wasn't involved with drugs at all, they will have features: \n",
    "1. POLICE-REPORTED_DRUG_INVOLVEMENT - No_Drugs\t\n",
    "2. METHOD_OF_DRUG_DETERMINATION - Not_Reported\n",
    "3. DRUG_TEST_TYPE - Not_Tested_for_Drugs\n",
    "4. DRUG_TEST_RESULTS_(1_of_3) - 0 (for 65% of instances)\n",
    "5. DRUG_TEST_TYPE_(2_of_3) - Not_Tested_for_Drugs\n",
    "6. DRUG_TEST_RESULTS_(2_of_3) - 0 (for 88% of instances)\n",
    "7. DRUG_TEST_TYPE_(3_of_3) - Not_Tested_for_Drugs\n",
    "8. DRUG_TEST_RESULTS_(3_of_3) - 0 (for 88% of instances)\n",
    "\n",
    "Meaning they will have 8 redundant features contributing a lot of degrees-of-freedom that can help the model overfit the data.\n",
    "To preserve as much information as possible I decided to to keep\\make these columns:\n",
    "- ALCOHOL_TEST_TYPE - kept as is, 53% of values are \"Not_Tested_for_Alcohol\", not ideal but I'll keep for now\n",
    "- ALCOHOL_TEST_RESULT - kept as is\n",
    "- DRUG_TEST_COUNT - # of drug test conducted \n",
    "- DRUG_TEST_RESULTS_AVG - average score of tests conducted\n",
    "\n",
    "Police reports and drug test types will be dropped. I mostly feel bad for dropping drug test types because maybe they convey some information about the reliability of the test results; however, for at least 65% examples in the data set these columns are irrelevant, so we need to think about the what's best for the model overall. \n",
    "\n",
    "**Making the changes:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many drug tests were conducted\n",
    "df['DRUG_TEST_COUNT'] = (df[['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)']] != 0).sum(axis=1)\n",
    "# Averaging test results only excluding zeros by replacing them with NaN for the calculation\n",
    "df['DRUG_TEST_RESULTS_AVG'] = (df[['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)']].replace(0, np.NaN).mean(axis=1))\n",
    "# Returning NaN to Zeros \n",
    "df['DRUG_TEST_RESULTS_AVG'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop processed columns\n",
    "df.drop(columns=['DRUG_TEST_RESULTS_(1_of_3)','DRUG_TEST_RESULTS_(2_of_3)','DRUG_TEST_RESULTS_(3_of_3)',\n",
    "                 'POLICE-REPORTED_DRUG_INVOLVEMENT','METHOD_OF_DRUG_DETERMINATION','DRUG_TEST_TYPE',\n",
    "                 'DRUG_TEST_TYPE_(2_of_3)','DRUG_TEST_TYPE_(3_of_3)','POLICE_REPORTED_ALCOHOL_INVOLVEMENT',\n",
    "                 'METHOD_ALCOHOL_DETERMINATION'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HISPANIC_ORIGIN & RACE\n",
    "'HISPANICE_ORIGIN' is a column separated from 'RACE'. [It seems the USA has strict rules about these classifications](https://www.iowadatacenter.org/aboutdata/raceclassification). These columns however create a problem when the person in question is 'Not_a_Fatality_(Not_Applicable)' or 'Unknown', which is 76% of the time, and this creates a duplicate on both columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ethnic = df[['HISPANIC_ORIGIN', 'RACE']]\n",
    "df_ethnic.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I wanna do to address this problem is merge the two columns to 'RACE_ETHNICITY' and create a category 'Hispanic' for any non-negative category under 'HISPANIC_ORIGIN'. So a 'Central_or_South_American'+'White' will be overwritten as 'Hispanic'. (Sorry white central or south americans...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnics = df.HISPANIC_ORIGIN.unique() # Get a list of all the possible categories under 'HISPANIC_ORIGIN'\n",
    "\n",
    "# Keep only non-negative categories\n",
    "ethnics = [e for e in ethnics if e not in ('Not_a_Fatality_(Not_Applicable)', 'Non-Hispanic','Unknown')]\n",
    "df['HISPANIC_ORIGIN'] = df['HISPANIC_ORIGIN'].replace(ethnics, 'Hispanic')\n",
    "\n",
    "# I couldn't think of an elegant solution in pandas so I switched the categories with python tools\n",
    "RACE_ETHNICITY_zip = zip(list(df['HISPANIC_ORIGIN']), list(df['RACE']))\n",
    "RACE_ETHNICITY = []\n",
    "for tpl in RACE_ETHNICITY_zip:\n",
    "    if tpl[0] == 'Hispanic':\n",
    "        RACE_ETHNICITY.append(tpl[0])\n",
    "    else:\n",
    "        RACE_ETHNICITY.append(tpl[1])\n",
    "\n",
    "df['RACE_ETHNICITY'] = RACE_ETHNICITY     # Create the new column\n",
    "df.drop(columns=['HISPANIC_ORIGIN','RACE'], inplace=True) # Drop the merged columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the new column\n",
    "df['RACE_ETHNICITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELATED_FACTOR_(#)-PERSON_LEVEL\n",
    "These columns seem to be contain the same categories, which are comments about special circumstances in the accident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check number of distinct occurrences for values in a column - Continued\n",
    "df['RELATED_FACTOR_(1)-PERSON_LEVEL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate correlation between a certain category value and injury severity\n",
    "category = 'RELATED_FACTOR_(1)-PERSON_LEVEL'\n",
    "value = 'Improper_Crossing_or_Roadway_or_Intersection'\n",
    "dcorr = df.loc[df[category] == value]     # For specific category\n",
    "# dcorr = train_data.loc[df['column_name'].isin(some_values)] # For several categories\n",
    "\n",
    "print('Injury severity distribution for ' + category + \" - \" + str(value))\n",
    "dcorr['INJURY_SEVERITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are rarely used, but some convey strong information about the severity of injury as shown above, and I don't want to lose this information. I will approach this problem by one-hot-encoding the categories and then merging duplicates, but first I need to prepare the data for one-hot-encoding. \n",
    "\n",
    "## Thresholding Values in Columns by Counts\n",
    "A recurrent problem with this dataset are rare categories; one-hot-encoding will create a new feature for every rare category, and this will create many redundant parameters. I will deal with this by setting a count threshold. Categories that are counted more times than the  threshold will be kept as is, others will be labeled as 'rare'. The count threshold will be a hyperparamter for this model as it is hard to predict what will be the best value for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding the inital threshold to be 1% of dataset size\n",
    "tot_instances = df.shape[0]\n",
    "threshold = tot_instances*0.005\n",
    "print ('The minimum count threshold is: '+str(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the count threshold to all the categorical values\n",
    "obj_columns = list(df.select_dtypes(include=['object']).columns)    # Get a list of all the columns' names with object dtype\n",
    "obj_columns.remove('INJURY_SEVERITY')                               # If you chose to keep rare classes\n",
    "df = df.apply(lambda x: x.mask(x.map(x.value_counts())<threshold, 'RARE') if x.name in obj_columns else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "I have done all the feature engineering I think was appropriate for this dataset, now it's ready to be one-hot-encoded so we can:\n",
    "1. Merge the duplicate categories that came from RELATED_FACTOR_(#)-PERSON_LEVEL\n",
    "2. Feed the data to a machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One Hot Encode the categorical features in the dataset\n",
    "\n",
    "df_encoded = pd.get_dummies(data=df, columns=obj_columns)\n",
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge duplicates from RELATED_FACTOR_(#)-PERSON_LEVEL\n",
    "df_encoded['RELATED_FACTOR_RARE'] = df_encoded['RELATED_FACTOR_(1)-PERSON_LEVEL_RARE'] + df_encoded['RELATED_FACTOR_(2)-PERSON_LEVEL_RARE'] + df_encoded['RELATED_FACTOR_(3)-PERSON_LEVEL_RARE']\n",
    "df_encoded['RELATED_FACTOR_Not_Applicable'] = df_encoded['RELATED_FACTOR_(1)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'] + df_encoded['RELATED_FACTOR_(2)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'] + df_encoded['RELATED_FACTOR_(3)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons']\n",
    "# Drop what we merged\n",
    "df_encoded.drop(columns=['RELATED_FACTOR_(1)-PERSON_LEVEL_RARE',\n",
    "                 'RELATED_FACTOR_(2)-PERSON_LEVEL_RARE',\n",
    "                 'RELATED_FACTOR_(3)-PERSON_LEVEL_RARE', \n",
    "                 'RELATED_FACTOR_(1)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons',\n",
    "                 'RELATED_FACTOR_(2)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons',\n",
    "                 'RELATED_FACTOR_(3)-PERSON_LEVEL_Not_Applicable_-_Driver/None_-_All_Other_Persons'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the new features\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to X and Y for model fit\n",
    "X_orig = df_encoded.dropna()     # Remember that df is the train+test data so dropping NaN values will return the train data\n",
    "Y_orig = X_orig['INJURY_SEVERITY']      # Set the target column\n",
    "X_orig = X_orig.drop('INJURY_SEVERITY', axis=1)      # Remove the target column from the training data\n",
    "X_test = df_encoded[df_encoded['INJURY_SEVERITY'].isnull()]\n",
    "X_test = X_test.drop('INJURY_SEVERITY', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target classes to numbers with sklearn LabelEncoder if the target numbers don't matter\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(Y_orig)\n",
    "# le_name_mapping = dict(zip(le.transform(le.classes_), le.classes_))  # this will help us translate the predictions\n",
    "# Y_encoded = le.transform(Y_orig)\n",
    "# print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target classes to specific requested numbers (for competitions\\tests)\n",
    "class_dict = {'Possible_Injury': 0, 'No_Injury': 1, 'Fatal_Injury': 2,\n",
    "              'Nonincapaciting_Evident_Injury': 3, 'Incapaciting_Injury': 4}\n",
    "Y_orig = pd.DataFrame(Y_orig)\n",
    "Y_encoded = Y_orig.replace({'INJURY_SEVERITY' : class_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set to train and dev sets\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_orig, Y_encoded, test_size=0.2, random_state=42)\n",
    "# Note: train_test_split() already shuffles the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "**Phew! Finally the pre-processing of the data is complete!** <br>\n",
    "Now the dataset is ready to be fed to a machine learning algorithm of choice. Right now, XGBoost is considered to be the best general purpose algorithm when it comes to general regression and classification tasks. Since it doesn't seem like this problem could benefit from a deep neural network solution, I will stick with the popular choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert that data to the XGBoost specific DMatrix data format\n",
    "Dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "Ddev = xgb.DMatrix(X_dev, label=Y_dev)\n",
    "Dcv = xgb.DMatrix(X_orig, label=Y_encoded)   # For cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paramaters for the model\n",
    "num_class = len(np.unique(Y_train))\n",
    "param = {                              # General guidelines for initial paramaters:\n",
    "    'min_child_weight': 1,             # 1 (choose small for high class imbalance)\n",
    "    'gamma': 0.5,                        # 0.1-0.2\n",
    "    'lambda': 10,                       # L2 Regulariztion - default = 1\n",
    "    'scale_pos_weight': 1,             # 1 (choose small for high class imbalance)\n",
    "    'subsample': 0.6,                    # 0.5-0.9\n",
    "    'colsample_bytree': 0.8,             # 0.5-0.9\n",
    "    'colsample_bylevel': 0.7,              # 0.5-0.9\n",
    "    'max_depth': 5,                    # 3-10 \n",
    "    'eta': 0.1,                        # 0.05-0.3\n",
    "    'silent': 0,                       # 0 - prints progress    1 - quiet\n",
    "    'objective': 'multi:softmax',        \n",
    "    'num_class': num_class,             \n",
    "    'eval_metric': 'mlogloss'}  \n",
    "num_round = 1000                                      # the number of training iterations if not stopped early\n",
    "evallist = [(Dtrain, 'train'), (Ddev, 'eval')]        # Specify validation set to watch performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model on the training set to get an initial impression on the performance\n",
    "model = xgb.train(param, Dtrain, num_round, evallist, early_stopping_rounds=10)\n",
    "print(\"Best error: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model's Hyperparameters\n",
    "The default values of XGB paramaters do a decent job but maybe it's possible to improve the model's performance with some hyperparameter tuning. It's very difficult to tune a XGB model because many of the parameters are not orthogonal and changing one may effect the performance of another. I believe the best way to approach XGB tuning is via randomized grid search cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune the hyperparameters by random grid search cv (this should take about 2.5 hours)\n",
    "\n",
    "# clf = xgb.XGBClassifier()\n",
    "\n",
    "# param_grid = {\n",
    "#         'max_depth': [3, 5, 7, 9],\n",
    "#         'learning_rate': [0.1, 0.03, 0.01, 0.003],\n",
    "#         'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "#         'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "#         'colsample_bylevel': [0.6, 0.7, 0.8, 0.9],\n",
    "#         'min_child_weight': [0.5, 1.0, 3.0, 5.0],\n",
    "#         'gamma': [0, 0.25, 0.5, 1.0],\n",
    "#         'reg_lambda': [0.1, 1.0, 5.0, 10.0],\n",
    "#         'n_estimators': [300]}\n",
    "\n",
    "# fit_params = {'eval_metric': 'mlogloss',\n",
    "#               'early_stopping_rounds': 10,\n",
    "#               'eval_set': [(X_dev, Y_dev.values.ravel())]}\n",
    "\n",
    "# rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=10,\n",
    "#                             n_jobs=1, cv=2,\n",
    "#                             refit=False)\n",
    "\n",
    "# print(\"Randomized search..\")\n",
    "# search_time_start = time.time()\n",
    "# rs_clf.fit(X_train, Y_train.values.ravel(), **fit_params)\n",
    "# print(\"Randomized search time:\", time.time() - search_time_start)\n",
    "\n",
    "# best_score = rs_clf.best_score_\n",
    "# best_params = rs_clf.best_params_\n",
    "# print(\"Best score: {}\".format(best_score))\n",
    "# print(\"Best params: \")\n",
    "# for param_name in sorted(best_params.keys()):\n",
    "#     print('%s: %r' % (param_name, best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL - save the model for future predictions\n",
    "# joblib.dump(model, 'fars_model_v2.pkl', compress=False)\n",
    "# model = joblib.load('fars_model_v2.pkl') # to load it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "We tuned the model's parameters the best we could, now it's time to take a deeper look at the predictions it makes and see what kind of mistakes the model does to get inspiration on what more we can do to improve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimate error using XGBoost's own cross-validation\n",
    "# cv_results = xgb.cv(\n",
    "#     param,\n",
    "#     Dcv,\n",
    "#     num_boost_round=num_round,\n",
    "#     seed=42,\n",
    "#     nfold=5,\n",
    "#     metrics={'merror'},\n",
    "#     early_stopping_rounds=10)\n",
    "\n",
    "# cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the classes on the training set with the trained model\n",
    "train_pred = model.predict(Dtrain)\n",
    "\n",
    "# Predict the classes on the dev set\n",
    "dev_pred = model.predict(Ddev)\n",
    "\n",
    "# Check model accuracy on the dev and train set\n",
    "print ('Training Set Accuracy: ' + str((accuracy_score(Y_train, train_pred))))\n",
    "print ('Dev Set Accuracy: ' + str((accuracy_score(Y_dev, dev_pred))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81% percent accuracy is a pretty decent result. Why do I think so? I try to think about the expected performance of a human expert looking at this data, trying to predict the injury severity of an accident, and I imagine his accuracy to be similar. However, this is merely intuition.\n",
    "\n",
    "Accuracy is not an ideal metric for unbalanced classes like we have in this dataset, let's have a deeper look at the predictions the model made with a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the confusion matrix plot function\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_dev, dev_pred)\n",
    "print_confusion_matrix(cnf_matrix, class_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix, it seems the model classifies 'Fatal_Injury' (99.8%) and 'No_Injury' (98.0%) extremely well! But it struggles with the intermediate classes. The most problematic class for the algorithm to classify correctly is 'Possible Injury' (30.1%), I am very pleased about this result because it shows just how much it would've been difficult to correctly classify it without merging it with the 2 other classes!\n",
    "\n",
    "### How to Further Improve the Model\n",
    "I believe the most meaningful way to further improve this model is by doing better feature engineering. I made some choices to merge\\drop the columns, I decided on a 0.5% frequency threshold to drop the categories, performed one-hot-encoding on the categorical data. Maybe there was a better way to rearrange the data. Since I made this case-study for presentation purposes, and not for competition\\work, I will stop the process here.\n",
    "\n",
    "### Extracting Insights\n",
    "We have built a model that can predict injury severity when fed the appropriate features. We can derive even more valuable information about this field by looking at what the machine have learned!\n",
    "Let's see what we kind of applicable insights we can extract by examining the feature importance from the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 10 most important features in the model\n",
    "xgb.plot_importance(model, max_num_features = 10, importance_type='weight') # also useful to look at 'gain'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's great to see the feature importance correlates nicely with common sense. We see that **AGE** has the biggest impact on the model predictions. Other important features: **Alcohol use, medical care, restraint system and air bag deployment**. These are all very intuitive features that we (humans) also think are important factors. <br>\n",
    "- **'RACE_ETHNICITY_Not_a_Fatality...'** scores high on this list because it easily separates the non-'Fatal_Injury' from the rest of the classes\n",
    "- **TAKEN_TO_HOSPITAL** an intuitive filter for injury severity\n",
    "- **SEX_Female** was shown earlier to have significantly less fatal injuries and the algorithm picked up on that\n",
    "- **DRUG_TEST_COUNT** - this is a suprising appearance! I was sure that the drug test results would be more important but it seems the information about having several or none drug tests have more of an impact on prediction! Luckily I saved that information in this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We covered the FARS (Fatality Analysis Reporting System) data set which contained statistics about car accidents made by the U.S. National Center for Statistics and Analysis. The data set contained a mix of numeric and categorical data. It was structured in a way that was supposed to be ergonomic for the human user but it was a difficult format for machine learning.\n",
    "\n",
    "In this case study we used the following machine learning techniques: \n",
    "- **Feature Engineering:** dropping overlapping classes, synthetizing new columns by merging existing columns, dropping columns that repeated information or contained information that was too specific, setting a threshold for distinct values count.\n",
    "- **One Hot Encoding:** categorical data was handled using OHE, this technique was also used as a solution to merge duplicate categories from different columns\n",
    "- **XGBoost:** we utillized an optimized distributed gradient boosting library, which implements machine learning algorithms under the Gradient Boosting framework.\n",
    "- **Hyperparameter Tuning via RandomizedSearchCV**\n",
    "- ***Quantitative* and *Qualitative*** error analysis on the model's prediction and mistakes\n",
    "\n",
    "We have learned applicable insights from the data about important factors for car accident related injury: <br>\n",
    "**Age** is the most dominant characteristic for determining injury severity in car accidents followed by **Alcohol\\Drug involvement, gender, restraint system usage and air bag deployment**\n",
    "\n",
    "These insights can be used for business purposes (Insurance companies policies) or education!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## About the Author\n",
    "Hello! I hope you enjoyed this case study!\n",
    "\n",
    "My name is Dan Bochman, <br>\n",
    "I enjoy making content that can help fellow data science enthusiasts get into the field. I'm trying to put the focus on topics that were difficult for me to understand, and that I didn't find much sources to learn from on them.\n",
    "\n",
    "If you have any questions for me regarding this work, or just want to contact me you can:  <br>\n",
    "Send me an e-mail at: dannybochman@gmail.com <br>\n",
    "Connect with me on [LinkedIn](https://www.linkedin.com/in/danbochman/) <br>\n",
    "Check out my [GitHub account](https://github.com/danbochman) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
